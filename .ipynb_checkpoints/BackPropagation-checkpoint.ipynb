{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08055f9b-c23e-4b1c-a80e-78eaadc2e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        # Hidden layer\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = self.sigmoid(self.Z1)\n",
    "\n",
    "        # Output layer\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "\n",
    "        return self.A2\n",
    "\n",
    "    def compute_loss(self, predicted, actual):\n",
    "        # Binary cross-entropy loss\n",
    "        m = actual.shape[1]\n",
    "        loss = -np.sum(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted)) / m\n",
    "        return loss\n",
    "\n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "\n",
    "        # Output layer gradients\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = np.dot(dZ2, self.A1.T) / m\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Hidden layer gradients\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * self.sigmoid_derivative(self.A1)\n",
    "        dW1 = np.dot(dZ1, X.T) / m\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        # Update weights and biases\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            predicted = self.forward_propagation(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(predicted, Y)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "# Generate synthetic binary classification dataset\n",
    "def generate_data():\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(2, 400)\n",
    "    Y = np.zeros((1, 400))\n",
    "\n",
    "    # Create two clusters\n",
    "    Y[0, :200] = 1  # First half labeled as 1\n",
    "    X[0, :200] += 2  # Shift first cluster\n",
    "    X[1, :200] += 2\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "def main():\n",
    "    # Generate dataset\n",
    "    X, Y = generate_data()\n",
    "\n",
    "    # Create and train neural network\n",
    "    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "    loss_history = nn.train(X, Y, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot loss history\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X[0, :200], X[1, :200], c='red', label='Class 0')\n",
    "    plt.scatter(X[0, 200:], X[1, 200:], c='blue', label='Class 1')\n",
    "\n",
    "    # Create mesh to plot decision boundary\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    # Predict for mesh points\n",
    "    Z = []\n",
    "    for x1, x2 in zip(xx.ravel(), yy.ravel()):\n",
    "        x = np.array([[x1], [x2]])\n",
    "        Z.append(nn.forward_propagation(x)[0, 0])\n",
    "    Z = np.array(Z).reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
